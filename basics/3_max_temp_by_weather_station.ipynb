{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Maximum Temperature by Weather Station\n",
    "\n",
    "The data used for this exercise can be found at datasets/1800.csv. This is real weather data form 1800, each of its rows represents a weather measurment. Among the data fields we can find the id of the weather station that made the measurment, the date, the observation type and the measured temperature (in tenths of degrees celsius). The data contains other fields, but we'll just ignore them, for they are not useful in our analysis.\n",
    "\n",
    "In this notebook we use PySpark is to find the maximum temperature measured by each weather station in 1800. \n",
    "\n",
    "First we call the some libraries and tell the computer that we are going to run the script on our local system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark import SparkConf, SparkContext\n",
    "\n",
    "conf = SparkConf().setMaster(\"local\").setAppName(\"num_friends_by_age\")\n",
    "sc = SparkContext(conf = conf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's load the data and display a little sample of it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "ITE00100554,18000101,TMAX,-75,,,E,\nITE00100554,18000101,TMIN,-148,,,E,\nGM000010962,18000101,PRCP,0,,,E,\nEZE00100082,18000101,TMAX,-86,,,E,\nEZE00100082,18000101,TMIN,-135,,,E,\nITE00100554,18000102,TMAX,-60,,I,E,\nITE00100554,18000102,TMIN,-125,,,E,\nGM000010962,18000102,PRCP,0,,,E,\nEZE00100082,18000102,TMAX,-44,,,E,\nEZE00100082,18000102,TMIN,-130,,,E,\n"
    }
   ],
   "source": [
    "raw_data = sc.textFile('datasets/1800.csv')\n",
    "\n",
    "for row in raw_data.take(10):\n",
    "    print(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we define a function that will help us to split the rows into the fields and to keep only the ones that are useful for us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(row):\n",
    "    splitted_row = row.split(',')\n",
    "    # We onply keep the following fields\n",
    "    station_id = splitted_row[0]\n",
    "    obs_type = splitted_row[2]\n",
    "    # Converts the temperature to regular degrees celsius\n",
    "    temperature = float(splitted_row[3]) / 10\n",
    "    return (station_id, obs_type, temperature)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next cell we create a new RDD which contains only the fields choosen by the split_data function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "('ITE00100554', 'TMAX', -7.5)\n('ITE00100554', 'TMIN', -14.8)\n('GM000010962', 'PRCP', 0.0)\n('EZE00100082', 'TMAX', -8.6)\n('EZE00100082', 'TMIN', -13.5)\n('ITE00100554', 'TMAX', -6.0)\n('ITE00100554', 'TMIN', -12.5)\n('GM000010962', 'PRCP', 0.0)\n('EZE00100082', 'TMAX', -4.4)\n('EZE00100082', 'TMIN', -13.0)\n"
    }
   ],
   "source": [
    "data = raw_data.map(split_data)\n",
    "\n",
    "for row in data.take(10):\n",
    "    print(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next block of code flilters out all the rows that don't correspond to maximum temperature observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "only_max_temps = data.filter(lambda measure: 'TMAX' in measure)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we don't need the observation type field, thus we can get rid of it and transform the values of our RDD into key/values pairs, where the key is the weather station id and the value the temperature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "only_max_temps = only_max_temps.map(lambda measurment: (measurment[0], measurment[2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we are able to find the maximum temperature for each weather station"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_temps = only_max_temps.reduceByKey(lambda temp_1, temp_2: max(temp_1, temp_2))\n",
    "results = max_temps.collect() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's print our results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "('ITE00100554', 32.3)\n('EZE00100082', 32.3)\n"
    }
   ],
   "source": [
    "for result in results:\n",
    "    print(result)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "spark",
   "display_name": "spark"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}